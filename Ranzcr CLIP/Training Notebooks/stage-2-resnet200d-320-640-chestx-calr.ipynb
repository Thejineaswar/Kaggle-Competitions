{"cells":[{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer.git\n!pip install adamp\nimport sys\nsys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\nsys.path.append('./Ranger-Deep-Learning-Optimizer/ranger')","execution_count":2,"outputs":[{"output_type":"stream","text":"fatal: destination path 'Ranger-Deep-Learning-Optimizer' already exists and is not an empty directory.\nCollecting adamp\n  Using cached adamp-0.3.0-py3-none-any.whl\nInstalling collected packages: adamp\nSuccessfully installed adamp-0.3.0\n","name":"stdout"}]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\nfrom pprint import pprint\nimport cv2, glob, time, random, os, ast, random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport timm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, CosineAnnealingLR, OneCycleLR\nfrom torch.optim import Adam, AdamW, SGD\n\nimport albumentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom adamp import AdamP\nfrom ranger import Ranger  # this is from ranger.py\nfrom ranger913A import RangerVA  # this is from ranger913A.py\nfrom rangerqh import RangerQH  # this is from rangerqh.py\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"# get the list of pretrained models\nmodel_names = timm.list_models()\npprint(model_names)","execution_count":4,"outputs":[{"output_type":"stream","text":"['adv_inception_v3',\n 'cspdarknet53',\n 'cspdarknet53_iabn',\n 'cspresnet50',\n 'cspresnet50d',\n 'cspresnet50w',\n 'cspresnext50',\n 'cspresnext50_iabn',\n 'darknet53',\n 'densenet121',\n 'densenet121d',\n 'densenet161',\n 'densenet169',\n 'densenet201',\n 'densenet264',\n 'densenet264d_iabn',\n 'densenetblur121d',\n 'dla34',\n 'dla46_c',\n 'dla46x_c',\n 'dla60',\n 'dla60_res2net',\n 'dla60_res2next',\n 'dla60x',\n 'dla60x_c',\n 'dla102',\n 'dla102x',\n 'dla102x2',\n 'dla169',\n 'dpn68',\n 'dpn68b',\n 'dpn92',\n 'dpn98',\n 'dpn107',\n 'dpn131',\n 'eca_vovnet39b',\n 'ecaresnet18',\n 'ecaresnet50',\n 'ecaresnet50d',\n 'ecaresnet50d_pruned',\n 'ecaresnet101d',\n 'ecaresnet101d_pruned',\n 'ecaresnetlight',\n 'ecaresnext26tn_32x4d',\n 'efficientnet_b0',\n 'efficientnet_b1',\n 'efficientnet_b1_pruned',\n 'efficientnet_b2',\n 'efficientnet_b2_pruned',\n 'efficientnet_b2a',\n 'efficientnet_b3',\n 'efficientnet_b3_pruned',\n 'efficientnet_b3a',\n 'efficientnet_b4',\n 'efficientnet_b5',\n 'efficientnet_b6',\n 'efficientnet_b7',\n 'efficientnet_b8',\n 'efficientnet_cc_b0_4e',\n 'efficientnet_cc_b0_8e',\n 'efficientnet_cc_b1_8e',\n 'efficientnet_el',\n 'efficientnet_em',\n 'efficientnet_es',\n 'efficientnet_l2',\n 'efficientnet_lite0',\n 'efficientnet_lite1',\n 'efficientnet_lite2',\n 'efficientnet_lite3',\n 'efficientnet_lite4',\n 'ens_adv_inception_resnet_v2',\n 'ese_vovnet19b_dw',\n 'ese_vovnet19b_slim',\n 'ese_vovnet19b_slim_dw',\n 'ese_vovnet39b',\n 'ese_vovnet39b_evos',\n 'ese_vovnet57b',\n 'ese_vovnet99b',\n 'ese_vovnet99b_iabn',\n 'fbnetc_100',\n 'gluon_inception_v3',\n 'gluon_resnet18_v1b',\n 'gluon_resnet34_v1b',\n 'gluon_resnet50_v1b',\n 'gluon_resnet50_v1c',\n 'gluon_resnet50_v1d',\n 'gluon_resnet50_v1s',\n 'gluon_resnet101_v1b',\n 'gluon_resnet101_v1c',\n 'gluon_resnet101_v1d',\n 'gluon_resnet101_v1s',\n 'gluon_resnet152_v1b',\n 'gluon_resnet152_v1c',\n 'gluon_resnet152_v1d',\n 'gluon_resnet152_v1s',\n 'gluon_resnext50_32x4d',\n 'gluon_resnext101_32x4d',\n 'gluon_resnext101_64x4d',\n 'gluon_senet154',\n 'gluon_seresnext50_32x4d',\n 'gluon_seresnext101_32x4d',\n 'gluon_seresnext101_64x4d',\n 'gluon_xception65',\n 'hrnet_w18',\n 'hrnet_w18_small',\n 'hrnet_w18_small_v2',\n 'hrnet_w30',\n 'hrnet_w32',\n 'hrnet_w40',\n 'hrnet_w44',\n 'hrnet_w48',\n 'hrnet_w64',\n 'ig_resnext101_32x8d',\n 'ig_resnext101_32x16d',\n 'ig_resnext101_32x32d',\n 'ig_resnext101_32x48d',\n 'inception_resnet_v2',\n 'inception_v3',\n 'inception_v4',\n 'legacy_senet154',\n 'legacy_seresnet18',\n 'legacy_seresnet34',\n 'legacy_seresnet50',\n 'legacy_seresnet101',\n 'legacy_seresnet152',\n 'legacy_seresnext26_32x4d',\n 'legacy_seresnext50_32x4d',\n 'legacy_seresnext101_32x4d',\n 'mixnet_l',\n 'mixnet_m',\n 'mixnet_s',\n 'mixnet_xl',\n 'mixnet_xxl',\n 'mnasnet_050',\n 'mnasnet_075',\n 'mnasnet_100',\n 'mnasnet_140',\n 'mnasnet_a1',\n 'mnasnet_b1',\n 'mnasnet_small',\n 'mobilenetv2_100',\n 'mobilenetv2_110d',\n 'mobilenetv2_120d',\n 'mobilenetv2_140',\n 'mobilenetv3_large_075',\n 'mobilenetv3_large_100',\n 'mobilenetv3_rw',\n 'mobilenetv3_small_075',\n 'mobilenetv3_small_100',\n 'nasnetalarge',\n 'pnasnet5large',\n 'regnetx_002',\n 'regnetx_004',\n 'regnetx_006',\n 'regnetx_008',\n 'regnetx_016',\n 'regnetx_032',\n 'regnetx_040',\n 'regnetx_064',\n 'regnetx_080',\n 'regnetx_120',\n 'regnetx_160',\n 'regnetx_320',\n 'regnety_002',\n 'regnety_004',\n 'regnety_006',\n 'regnety_008',\n 'regnety_016',\n 'regnety_032',\n 'regnety_040',\n 'regnety_064',\n 'regnety_080',\n 'regnety_120',\n 'regnety_160',\n 'regnety_320',\n 'res2net50_14w_8s',\n 'res2net50_26w_4s',\n 'res2net50_26w_6s',\n 'res2net50_26w_8s',\n 'res2net50_48w_2s',\n 'res2net101_26w_4s',\n 'res2next50',\n 'resnest14d',\n 'resnest26d',\n 'resnest50d',\n 'resnest50d_1s4x24d',\n 'resnest50d_4s2x40d',\n 'resnest101e',\n 'resnest200e',\n 'resnest269e',\n 'resnet18',\n 'resnet18d',\n 'resnet26',\n 'resnet26d',\n 'resnet34',\n 'resnet34d',\n 'resnet50',\n 'resnet50d',\n 'resnet101',\n 'resnet101d',\n 'resnet152',\n 'resnet152d',\n 'resnet200',\n 'resnet200d',\n 'resnet200d_320',\n 'resnetblur18',\n 'resnetblur50',\n 'resnext50_32x4d',\n 'resnext50d_32x4d',\n 'resnext101_32x4d',\n 'resnext101_32x8d',\n 'resnext101_64x4d',\n 'rexnet_100',\n 'rexnet_130',\n 'rexnet_150',\n 'rexnet_200',\n 'rexnetr_100',\n 'rexnetr_130',\n 'rexnetr_150',\n 'rexnetr_200',\n 'selecsls42',\n 'selecsls42b',\n 'selecsls60',\n 'selecsls60b',\n 'selecsls84',\n 'semnasnet_050',\n 'semnasnet_075',\n 'semnasnet_100',\n 'semnasnet_140',\n 'senet154',\n 'seresnet18',\n 'seresnet34',\n 'seresnet50',\n 'seresnet50tn',\n 'seresnet101',\n 'seresnet152',\n 'seresnet152d',\n 'seresnext26_32x4d',\n 'seresnext26d_32x4d',\n 'seresnext26t_32x4d',\n 'seresnext26tn_32x4d',\n 'seresnext50_32x4d',\n 'seresnext101_32x4d',\n 'seresnext101_32x8d',\n 'skresnet18',\n 'skresnet34',\n 'skresnet50',\n 'skresnet50d',\n 'skresnext50_32x4d',\n 'spnasnet_100',\n 'ssl_resnet18',\n 'ssl_resnet50',\n 'ssl_resnext50_32x4d',\n 'ssl_resnext101_32x4d',\n 'ssl_resnext101_32x8d',\n 'ssl_resnext101_32x16d',\n 'swsl_resnet18',\n 'swsl_resnet50',\n 'swsl_resnext50_32x4d',\n 'swsl_resnext101_32x4d',\n 'swsl_resnext101_32x8d',\n 'swsl_resnext101_32x16d',\n 'tf_efficientnet_b0',\n 'tf_efficientnet_b0_ap',\n 'tf_efficientnet_b0_ns',\n 'tf_efficientnet_b1',\n 'tf_efficientnet_b1_ap',\n 'tf_efficientnet_b1_ns',\n 'tf_efficientnet_b2',\n 'tf_efficientnet_b2_ap',\n 'tf_efficientnet_b2_ns',\n 'tf_efficientnet_b3',\n 'tf_efficientnet_b3_ap',\n 'tf_efficientnet_b3_ns',\n 'tf_efficientnet_b4',\n 'tf_efficientnet_b4_ap',\n 'tf_efficientnet_b4_ns',\n 'tf_efficientnet_b5',\n 'tf_efficientnet_b5_ap',\n 'tf_efficientnet_b5_ns',\n 'tf_efficientnet_b6',\n 'tf_efficientnet_b6_ap',\n 'tf_efficientnet_b6_ns',\n 'tf_efficientnet_b7',\n 'tf_efficientnet_b7_ap',\n 'tf_efficientnet_b7_ns',\n 'tf_efficientnet_b8',\n 'tf_efficientnet_b8_ap',\n 'tf_efficientnet_cc_b0_4e',\n 'tf_efficientnet_cc_b0_8e',\n 'tf_efficientnet_cc_b1_8e',\n 'tf_efficientnet_el',\n 'tf_efficientnet_em',\n 'tf_efficientnet_es',\n 'tf_efficientnet_l2_ns',\n 'tf_efficientnet_l2_ns_475',\n 'tf_efficientnet_lite0',\n 'tf_efficientnet_lite1',\n 'tf_efficientnet_lite2',\n 'tf_efficientnet_lite3',\n 'tf_efficientnet_lite4',\n 'tf_inception_v3',\n 'tf_mixnet_l',\n 'tf_mixnet_m',\n 'tf_mixnet_s',\n 'tf_mobilenetv3_large_075',\n 'tf_mobilenetv3_large_100',\n 'tf_mobilenetv3_large_minimal_100',\n 'tf_mobilenetv3_small_075',\n 'tf_mobilenetv3_small_100',\n 'tf_mobilenetv3_small_minimal_100',\n 'tresnet_l',\n 'tresnet_l_448',\n 'tresnet_m',\n 'tresnet_m_448',\n 'tresnet_xl',\n 'tresnet_xl_448',\n 'tv_densenet121',\n 'tv_resnet34',\n 'tv_resnet50',\n 'tv_resnet101',\n 'tv_resnet152',\n 'tv_resnext50_32x4d',\n 'vit_base_patch16_224',\n 'vit_base_patch16_384',\n 'vit_base_patch32_384',\n 'vit_base_resnet26d_224',\n 'vit_base_resnet50d_224',\n 'vit_huge_patch16_224',\n 'vit_huge_patch32_384',\n 'vit_large_patch16_224',\n 'vit_large_patch16_384',\n 'vit_large_patch32_384',\n 'vit_small_patch16_224',\n 'vit_small_resnet26d_224',\n 'vit_small_resnet50d_s3_224',\n 'vovnet39a',\n 'vovnet57a',\n 'wide_resnet50_2',\n 'wide_resnet101_2',\n 'xception',\n 'xception41',\n 'xception65',\n 'xception71']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"cont\"></a>\n## CFG"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"BATCH_SIZE = 8 # 8 for bigger architectures\nVAL_BATCH_SIZE = 16\nEPOCHS = 13 # train upto 10 epochs\nIMG_SIZE = 640 # 384 for bigger architectures\nif BATCH_SIZE == 8:\n    ITER_FREQ = 400\nelse:\n    ITER_FREQ = 200\nNUM_WORKERS = 8\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\nSEED = 1111\nN_FOLDS = 5\nTR_FOLDS = [0,1,2,3,4]\nSTART_FOLD = 0\n\ntarget_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n                 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', \n                 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n                 'Swan Ganz Catheter Present']\n\nMODEL_PATH = '../input/stage2-8th-cv-9449/Stage2_resnet200d_320_fold_0_epoch_8.pth'\nMODEL_ARCH = 'resnet200d_320' # tf_efficientnet_b4_ns, tf_efficientnet_b5_ns, resnext50_32x4d, seresnet152d\nTEACHER_MODEL_PATH = '../input/stage1-resnet200d-320/stage1_resnet200d_320_fold_4_epoch_4.pth'\nWEIGHTS = [0.5, 1]\n\nLR = 5e-4\nMIN_LR = 1e-6 # SAM, CosineAnnealingWarmRestarts\nWEIGHT_DECAY = 1e-6\nMOMENTUM = 0.9\nT_0 = EPOCHS # SAM, CosineAnnealingWarmRestarts\nMAX_NORM = 1000\nT_MAX = 5\nITERS_TO_ACCUMULATE = 1\n\nBASE_OPTIMIZER = SGD #for SAM, Ranger\nOPTIMIZER = 'Ranger' # Ranger, AdamW, AdamP, SGD, SAM\n\nSCHEDULER = 'CosineAnnealingLR' # ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts, OneCycleLR\nSCHEDULER_UPDATE = 'epoch' # batch\n\nCRITERION = 'BCE' # CrossEntropyLoss, TaylorSmoothedLoss, LabelSmoothedLoss\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n        \n    def reset(self):\n        self.val = 0\n        self.sum = 0\n        self.avg = 0\n        self.count = 0\n        \n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val*n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef seed_torch(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(SEED)\n\ndef macro_multilabel_auc(label, pred):\n    aucs = []\n    for i in range(len(target_cols)):\n        aucs.append(roc_auc_score(label[:, i], pred[:, i]))\n    print(np.round(aucs, 4))\n    return np.mean(aucs)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"TRAIN_DIR = '../input/ranzcr-clip-catheter-line-classification/train/'\ntrain_df = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/train.csv')\nfolds = pd.read_csv('../input/ranzcr-folds/train_folds.csv')\ntrain_annotations = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/train_annotations.csv')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"COLOR_MAP = {'ETT - Abnormal': (255, 0, 0),\n             'ETT - Borderline': (0, 255, 0),\n             'ETT - Normal': (0, 0, 255),\n             'NGT - Abnormal': (255, 255, 0),\n             'NGT - Borderline': (255, 0, 255),\n             'NGT - Incompletely Imaged': (0, 255, 255),\n             'NGT - Normal': (128, 0, 0),\n             'CVC - Abnormal': (0, 128, 0),\n             'CVC - Borderline': (0, 0, 128),\n             'CVC - Normal': (128, 128, 0),\n             'Swan Ganz Catheter Present': (128, 0, 128),\n            }\n\n\nclass RanzcrDataset(Dataset):\n    def __init__(self, df, df_annotations, use_annot=False, annot_size=50, transform=None):\n        self.df = df\n        self.df_annotations = df_annotations\n        self.use_annot = use_annot\n        self.annot_size = annot_size\n        self.image_id = df['StudyInstanceUID'].values\n        self.labels = df[target_cols].values\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image_id = self.image_id[idx]\n        file_path = f'{TRAIN_DIR}{image_id}.jpg'\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        labels = torch.tensor(self.labels[idx]).float()\n        if self.use_annot:\n            image_annot = image.copy()\n            query_string = f\"StudyInstanceUID == '{image_id}'\"\n            df = self.df_annotations.query(query_string)\n            for i, row in df.iterrows():\n                label = row[\"label\"]\n                data = np.array(ast.literal_eval(row[\"data\"]))\n                for d in data:\n                    image_annot[d[1]-self.annot_size//2:d[1]+self.annot_size//2,\n                                d[0]-self.annot_size//2:d[0]+self.annot_size//2,\n                                :] = COLOR_MAP[label]\n            if self.transform:\n                augmented = self.transform(image=image, image_annot=image_annot)\n                image = augmented['image']\n                image_annot = augmented['image_annot']\n            return image, image_annot, labels\n        else:\n            if self.transform:\n                augmented = self.transform(image=image)\n                image = augmented['image']\n            return image, labels","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def get_transform(*, train=True):\n    \n    if train:\n        return A.Compose([\n            A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, scale=(0.85, 1.0)),\n            A.HorizontalFlip(p=0.5),\n            A.RandomBrightnessContrast(p=0.2, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n            A.HueSaturationValue(p=0.2, hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2),\n            A.ShiftScaleRotate(p=0.2, shift_limit=0.0625, scale_limit=0.2, rotate_limit=20),\n            A.CoarseDropout(p=0.2),\n            A.Cutout(p=0.2, max_h_size=16, max_w_size=16, fill_value=(0., 0., 0.), num_holes=16),\n            A.Normalize(mean=MEAN, std=STD),\n            ToTensorV2(),\n        ], additional_targets={'image_annot': 'image'})\n    else:\n        return A.Compose([\n#             A.CenterCrop(IMG_SIZE, IMG_SIZE),\n            A.Resize(IMG_SIZE, IMG_SIZE),\n            A.Normalize(mean=MEAN, std=STD, max_pixel_value=255.0, p=1.0),\n            ToTensorV2(),\n        ])","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"class SeResnet152D(nn.Module): \n    def __init__(self, model_arch, n_classes, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_arch, pretrained=pretrained)\n        n_features = self.model.fc.in_features\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(n_features, n_classes)\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.fc(pooled_features)\n        return output\n\nclass CustomResNet200D(nn.Module):\n    def __init__(self, model_arch, n_classes, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_arch, pretrained=False)\n        n_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(n_features, n_classes)\n        if pretrained:\n            pretrained_path = '../input/startingpointschestx/resnet200d_320_chestx.pth'\n            checkpoint = torch.load(pretrained_path)['model']\n            for key in list(checkpoint.keys()):\n                if 'model.' in key:\n                    checkpoint[key.replace('model.', '')] = checkpoint[key]\n                    del checkpoint[key]\n            self.model.load_state_dict(checkpoint) \n            print(f'load {model_arch} pretrained model')\n        n_features = self.model.fc.in_features\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(n_features, n_classes)\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.fc(pooled_features)\n        return features, pooled_features, output","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self, weights=[1, 1]):\n        super(CustomLoss, self).__init__()\n        self.weights = weights\n        \n    def forward(self, teacher_features, features, y_pred, labels):\n        consistency_loss = nn.MSELoss()(teacher_features.view(-1), features.view(-1))\n        cls_loss = nn.BCEWithLogitsLoss()(y_pred, labels)\n        loss = self.weights[0] * consistency_loss + self.weights[1] * cls_loss\n        return loss","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Back to CFG(Click here)](#cont)"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def GetCriterion(criterion_name, criterion=None):\n\n    if criterion_name == 'CrossEntropyLoss':\n        criterion = nn.CrossEntropyLoss()\n    elif criterion_name == 'LabelSmoothingLoss':\n        criterion = LabelSmoothingLoss()\n#     elif criterion_name == 'FocalLoss':\n#         criterion = FocalLoss()\n#     elif criterion_name == 'FocalCosineLoss':\n#         criterion = FocalCosineLoss()\n    elif criterion_name == 'TaylorCrossEntropyLoss':\n        criterion = TaylorCrossEntropyLoss()\n    elif criterion_name == 'TaylorSmoothedLoss':\n        criterion = TaylorSmoothedLoss()\n    elif criterion_name == 'CutMix':\n        criterion = CutMixCriterion(criterion)\n    elif criterion_name == 'SnapMix':\n        criterion = SnapMixLoss()\n    elif criterion_name == 'CustomLoss':\n        criterion = CustomLoss(WEIGHTS)\n    elif criterion_name == 'BCE':\n        criterion = nn.BCEWithLogitsLoss()\n    return criterion\n    \n    \ndef GetScheduler(scheduler_name, optimizer, batches=None):\n    #['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts', 'OneCycleLR', 'GradualWarmupSchedulerV2']\n    if scheduler_name == 'OneCycleLR':\n        return torch.optim.lr_scheduler.OneCycleLR(optimizer,max_lr = 1e-2,epochs = EPOCHS,\n                                                   steps_per_epoch = batches+1,pct_start = 0.1)\n    if scheduler_name == 'CosineAnnealingWarmRestarts':\n        return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = T_0, T_mult=1,\n                                                                    eta_min=MIN_LR, last_epoch=-1)\n    elif scheduler_name == 'CosineAnnealingLR':\n        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_MAX, eta_min=0, last_epoch=-1)\n    elif scheduler_name == 'ReduceLROnPlateau':\n        return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=1, threshold=0.0001,\n                                                          cooldown=0, min_lr=MIN_LR)\n#     elif scheduler_name == 'GradualWarmupSchedulerV2':\n#         return GradualWarmupSchedulerV2(optimizer=optimizer)\n    \ndef GetOptimizer(optimizer_name,parameters):\n    #['Adam','Ranger']\n    if optimizer_name == 'Adam':\n#         if CFG.scheduler_name == 'GradualWarmupSchedulerV2':\n#             return torch.optim.Adam(parameters, lr=CFG.LR_START, weight_decay=CFG.weight_decay, amsgrad=False)\n#         else:\n        return torch.optim.Adam(parameters, lr=LR, weight_decay=WEIGHT_DECAY, amsgrad=False)\n    elif optimizer_name == 'AdamW':\n#         if CFG.scheduler_name == 'GradualWarmupSchedulerV2':\n#             return torch.optim.AdamW(parameters, lr=CFG.LR_START, weight_decay=CFG.weight_decay, amsgrad=False)\n#         else:\n        return torch.optim.Adam(parameters, lr=LR, weight_decay=WEIGHT_DECAY, amsgrad=False)\n    elif optimizer_name == 'AdamP':\n#         if CFG.scheduler_name == 'GradualWarmupSchedulerV2':\n#             return AdamP(parameters, lr=CFG.LR_START, weight_decay=CFG.weight_decay)\n#         else:\n        return AdamP(parameters, lr=LR, weight_decay=WEIGHT_DECAY)\n    elif optimizer_name == 'Ranger':\n        return Ranger(parameters, lr = LR, alpha = 0.5, k = 6, N_sma_threshhold = 5, \n                      betas = (0.95,0.999), weight_decay=WEIGHT_DECAY)\n    elif optimizer_name == 'SAM':\n        return SAM(parameters, BASE_OPTIMIZER, lr=0.1, momentum=0.9,weight_decay=0.0005)\n    \n    elif optimizer_name == 'AdamP':\n        return AdamP(parameters, lr=LR, weight_decay=WEIGHT_DECAY)","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and validation functions"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def train_fn(model, dataloader, teacher_model, device, epoch, optimizer, criterion, scheduler):\n    \n    data_time = AverageMeter()\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n    \n    model.train()\n    # https://pytorch.org/docs/stable/notes/amp_examples.html#gradient-accumulation\n    scaler = GradScaler()\n    start_time = time.time()\n    loader = tqdm(dataloader, total=len(dataloader))\n    for step, (images, images_annot, labels) in enumerate(loader):\n        \n        with torch.no_grad():\n            teacher_features, _, _ = teacher_model(images_annot.to(device))\n\n        images = images.to(device).float()\n        labels = labels.to(device)\n        data_time.update(time.time() - start_time)\n\n        with autocast():\n            features, _, output = model(images)\n            loss = criterion(teacher_features, features, output, labels)\n#             output = model(images)\n#             loss = criterion(output, labels)\n            losses.update(loss.item(), BATCH_SIZE)\n        \n            scaler.scale(loss).backward()\n            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm = MAX_NORM)\n\n            if (step+1) % ITERS_TO_ACCUMULATE == 0:\n                scaler.step(optimizer)\n                # Update the scale for next iteration.\n                scaler.update()\n                optimizer.zero_grad()\n        \n        if scheduler is not None and SCHEDULER_UPDATE == 'batch':\n            scheduler.step()\n\n        batch_time.update(time.time() - start_time)\n        start_time = time.time()\n        \n        if step % ITER_FREQ == 0:\n            \n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Batch Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s),\\t'\n                  'Data Time {data_time.val:.3f}s ({data_time.avg:.3f}s)\\t'\n                  'Loss: {loss.val:.4f} ({loss.avg:.4f})'.format((epoch+1),\n                                                                  step, len(dataloader),\n                                                                  batch_time=batch_time,\n                                                                  data_time=data_time,\n                                                                  loss=losses))\n                                                                             #accuracy=accuracies))\n        # To check the loss real-time while iterating over data.   'Accuracy {accuracy.val:.4f} ({accuracy.avg:.4f})'\n        loader.set_description(f'Training Epoch {epoch+1}/{EPOCHS}')\n        loader.set_postfix(loss=losses.avg) #accuracy=accuracies.avg)\n#         del images, labels\n    if scheduler is not None and SCHEDULER_UPDATE == 'epoch':\n        scheduler.step()\n        \n    return losses.avg#, accuracies.avg","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def valid_fn(epoch, model, criterion, val_loader, device, scheduler):\n    \n    model.eval()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n    PREDS = []\n    TARGETS = []\n    loader = tqdm(val_loader, total=len(val_loader))\n    with torch.no_grad():  # without torch.no_grad() will make the CUDA run OOM.\n        for step, (images, labels) in enumerate(loader):\n\n            images = images.to(device)\n            labels = labels.to(device)\n\n#             output = model(images)\n            _, _, output = model(images)\n            loss = criterion(output, labels)\n            losses.update(loss.item(), BATCH_SIZE)\n            PREDS += [output.sigmoid()]\n            TARGETS += [labels.detach().cpu()]\n#             accuracy = (F.softmax(output).argmax(dim=1) == labels).float().mean()\n#             accuracies.update(accuracy.item(), VAL_BATCH_SIZE)\n            loader.set_description(f'Validating Epoch {epoch+1}/{EPOCHS}')\n            loader.set_postfix(loss=losses.avg)#, accuracy=accuracies.avg)\n#             del images, labels\n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n    roc_auc = macro_multilabel_auc(TARGETS, PREDS)\n    if scheduler is not None:\n        scheduler.step()\n        \n    return losses.avg, roc_auc# accuracies.avg","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Back to CFG(Click here)](#cont)"},{"metadata":{},"cell_type":"markdown","source":"# Main"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def engine(device, folds, fold, model_path=None):\n    \n    trn_idx = folds[folds['kfold'] != fold].index\n    val_idx = folds[folds['kfold'] == fold].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n    train_folds = train_folds[train_folds['StudyInstanceUID'].isin(train_annotations['StudyInstanceUID'].unique())].reset_index(drop=True)\n\n    train_data = RanzcrDataset(train_folds, train_annotations, use_annot=True, transform=get_transform())\n    val_data = RanzcrDataset(valid_folds, train_annotations, use_annot=False, transform=get_transform(train=False))            \n    \n    train_loader = DataLoader(train_data,\n                              batch_size=BATCH_SIZE, \n                              shuffle=True, \n                              num_workers=NUM_WORKERS,\n                              pin_memory=True, # enables faster data transfer to CUDA-enabled GPUs.\n                              drop_last=True)\n    val_loader = DataLoader(val_data, \n                            batch_size=VAL_BATCH_SIZE,\n                            num_workers=NUM_WORKERS,\n                            shuffle=False, \n                            pin_memory=True,\n                            drop_last=False)\n    \n    teacher_model = CustomResNet200D(MODEL_ARCH, 11, pretrained=False)\n    teacher_model.load_state_dict(torch.load(TEACHER_MODEL_PATH))\n    for param in teacher_model.parameters():\n        param.requires_grad = False\n    teacher_model.eval()\n    teacher_model.to(device)\n    model = CustomResNet200D(MODEL_ARCH, 11, pretrained=True)\n    if model_path is not None:\n        model.load_state_dict(torch.load(model_path))\n        START_EPOCH = int((model_path.split('_')[-1]).split('.')[0])\n    else:\n        model = CustomResNet200D(MODEL_ARCH, 11, pretrained=True)\n        START_EPOCH = 0\n    model.to(device)\n    \n    params = filter(lambda p: p.requires_grad, model.parameters())    \n    optimizer = GetOptimizer(OPTIMIZER, params)\n    \n    train_criterion = CustomLoss(weights=WEIGHTS).to(device)        \n    val_criterion = GetCriterion(CRITERION).to(device)\n    \n    scheduler = GetScheduler(SCHEDULER, optimizer, BATCH_SIZE)\n    \n    loss = []\n    accuracy = []\n    for epoch in range(START_EPOCH, EPOCHS):\n        \n        epoch_start = time.time()        \n        avg_loss = train_fn(model, train_loader, teacher_model, device, epoch, optimizer, train_criterion, scheduler)\n\n        torch.cuda.empty_cache()\n        avg_val_loss, roc_auc_score = valid_fn(epoch, model, val_criterion, val_loader, device, scheduler)\n        epoch_end = time.time() - epoch_start\n        \n        print(f'Validation ROC AUC Score after epoch {epoch+1}: {roc_auc_score:.4f}')\n        loss.append(avg_loss)\n#         accuracy.append(avg_accuracy)\n        \n        content = f'Fold {fold} Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} roc_auc_score: {roc_auc_score:.4f} time: {epoch_end:.0f}s'\n        with open(f'Stage2_{MODEL_ARCH}_{OPTIMIZER}_{CRITERION}.txt', 'a') as appender:\n            appender.write(content + '\\n')                                         # avg_train_accuracy: {avg_accuracy:.4f}\n        \n\n        torch.save(model.state_dict(), f'Stage2_{MODEL_ARCH}_fold_{fold}_epoch_{(epoch+1)}.pth')\n#         torch.save(model, f'GPU_{MODEL_ARCH}_fold_{fold}_epoch_{(epoch+1)}')\n        torch.cuda.empty_cache()\n    \n    return loss#{'loss':loss, 'accuracy':accuracy}","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"if __name__ == '__main__':\n    \n    if MODEL_PATH is not None:\n        START_FOLD = int(MODEL_PATH.split('_')[-3])\n    \n    for fold in range(START_FOLD, N_FOLDS):\n        if fold == 1:\n            break\n        print(f'===== Fold {fold} Starting =====')\n        fold_start = time.time()\n        logs = engine(DEVICE, folds, fold, MODEL_PATH)\n        print(f'Time taken in fold {fold}: {time.time()-fold_start}')","execution_count":16,"outputs":[{"output_type":"stream","text":"===== Fold 0 Starting =====\nload resnet200d_320 pretrained model\nRanger optimizer loaded. \nGradient Centralization usage = True\nGC applied to both conv and fc layers\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/900 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24707dae2b73430cb7db2782b9be680f"}},"metadata":{}},{"output_type":"stream","text":"Epoch: [9][0/900]\tBatch Time 13.657s (13.657s),\tData Time 10.906s (10.906s)\tLoss: 1.8372 (1.8372)\nEpoch: [9][400/900]\tBatch Time 1.748s (1.749s),\tData Time 0.385s (0.412s)\tLoss: 1.8969 (2.5320)\nEpoch: [9][800/900]\tBatch Time 1.686s (1.734s),\tData Time 0.384s (0.399s)\tLoss: 2.0083 (2.4886)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/377 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d38c54ec9b654e9cbdba2c3642b15727"}},"metadata":{}},{"output_type":"stream","text":"[0.9913 0.943  0.9851 0.9309 0.9373 0.98   0.9843 0.9016 0.8317 0.8939\n 0.991 ]\nValidation ROC AUC Score after epoch 9: 0.9427\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/900 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"727d5ca8bfe64119b0a28feb30f7c90c"}},"metadata":{}},{"output_type":"stream","text":"Epoch: [10][0/900]\tBatch Time 11.887s (11.887s),\tData Time 7.220s (7.220s)\tLoss: 3.3960 (3.3960)\nEpoch: [10][400/900]\tBatch Time 1.688s (1.744s),\tData Time 0.385s (0.404s)\tLoss: 1.6818 (2.3861)\nEpoch: [10][800/900]\tBatch Time 1.688s (1.730s),\tData Time 0.385s (0.395s)\tLoss: 3.7827 (2.4237)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/377 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a70147a20394371b7a79641e63ef763"}},"metadata":{}},{"output_type":"stream","text":"[0.9843 0.9519 0.9811 0.9161 0.937  0.9828 0.9839 0.9089 0.8284 0.8924\n 0.9968]\nValidation ROC AUC Score after epoch 10: 0.9421\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/900 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f63c4c6d86429db55a38d75783b80d"}},"metadata":{}},{"output_type":"stream","text":"Epoch: [11][0/900]\tBatch Time 12.587s (12.587s),\tData Time 8.366s (8.366s)\tLoss: 1.5569 (1.5569)\nEpoch: [11][400/900]\tBatch Time 1.713s (1.748s),\tData Time 0.385s (0.407s)\tLoss: 2.6164 (2.4866)\nEpoch: [11][800/900]\tBatch Time 1.718s (1.735s),\tData Time 0.384s (0.397s)\tLoss: 8.1711 (2.4093)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/377 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d738c5e671406985704dca9d10d9d9"}},"metadata":{}},{"output_type":"stream","text":"[0.982  0.9495 0.9746 0.9161 0.9269 0.9821 0.9808 0.91   0.8329 0.8901\n 0.9967]\nValidation ROC AUC Score after epoch 11: 0.9401\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/900 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dffb4dea79a4c78a7043d1ae4284975"}},"metadata":{}},{"output_type":"stream","text":"Epoch: [12][0/900]\tBatch Time 10.991s (10.991s),\tData Time 7.064s (7.064s)\tLoss: 1.4934 (1.4934)\nEpoch: [12][400/900]\tBatch Time 1.833s (1.743s),\tData Time 0.384s (0.404s)\tLoss: 5.3226 (2.4103)\nEpoch: [12][800/900]\tBatch Time 1.806s (1.731s),\tData Time 0.384s (0.395s)\tLoss: 0.7345 (2.4364)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/377 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab23f18b95a842f787f2df887dd8a1d1"}},"metadata":{}},{"output_type":"stream","text":"[0.9689 0.9411 0.9845 0.9056 0.9259 0.98   0.9826 0.9103 0.8349 0.8995\n 0.9991]\nValidation ROC AUC Score after epoch 12: 0.9393\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/900 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"690a5d36729a473b98d62e61c0f0b48f"}},"metadata":{}},{"output_type":"stream","text":"Epoch: [13][0/900]\tBatch Time 9.777s (9.777s),\tData Time 6.739s (6.739s)\tLoss: 4.7465 (4.7465)\nEpoch: [13][400/900]\tBatch Time 1.819s (1.739s),\tData Time 0.384s (0.402s)\tLoss: 0.9148 (2.4646)\nEpoch: [13][800/900]\tBatch Time 1.689s (1.729s),\tData Time 0.384s (0.394s)\tLoss: 2.2120 (2.4352)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/377 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"609406e5ff93452cad737c3898a61ad4"}},"metadata":{}},{"output_type":"stream","text":"[0.9827 0.9506 0.9862 0.8992 0.9287 0.9826 0.9853 0.9147 0.8326 0.8999\n 0.6693]\nValidation ROC AUC Score after epoch 13: 0.9120\nTime taken in fold 0: 9770.604644298553\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}